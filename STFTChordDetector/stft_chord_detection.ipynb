{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chord Detection via FFT\n",
    "Creates a 9:16 vertical video showing real-time chord detection from audio using Fourier transform and chromagram analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.patches import Rectangle\n",
    "import imageio\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "AUDIO_PATH = Path(\"audio/input.mp3\")\n",
    "OUTPUT_PATH = Path(\"out/chord_detection.mp4\")\n",
    "\n",
    "FPS = 30#30\n",
    "WINDOW_SIZE = 8192 \n",
    "HOP_SIZE = 512    \n",
    "\n",
    "# Frequency range for analysis (guitar fundamentals)\n",
    "FREQ_MIN = 60   # Hz (below low E ~82Hz)\n",
    "FREQ_MAX = 1000 # Hz (covers harmonics)\n",
    "\n",
    "# Detection threshold (0-1, normalized chroma)\n",
    "DETECTION_THRESHOLD = 0.3\n",
    "\n",
    "# Volume threshold - don't detect chords below this RMS level\n",
    "VOLUME_THRESHOLD = 0.02\n",
    "\n",
    "# Waveform display window (in seconds)\n",
    "WAVEFORM_WINDOW = 0.1  # 100ms of audio displayed at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24 chord templates\n",
      "Chords: ['C', 'Cm', 'C#', 'C#m', 'D', 'Dm', 'D#', 'D#m', 'E', 'Em', 'F', 'Fm', 'F#', 'F#m', 'G', 'Gm', 'G#', 'G#m', 'A', 'Am', 'A#', 'A#m', 'B', 'Bm']\n"
     ]
    }
   ],
   "source": [
    "# Note names and chord templates\n",
    "NOTE_NAMES = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "\n",
    "def make_chord_template(root: int, intervals: list) -> np.ndarray:\n",
    "    \"\"\"Create a chord template given root pitch class and intervals.\"\"\"\n",
    "    template = np.zeros(12)\n",
    "    for interval in intervals:\n",
    "        template[(root + interval) % 12] = 1.0\n",
    "    return template\n",
    "\n",
    "# Intervals: major = [0, 4, 7], minor = [0, 3, 7]\n",
    "MAJOR_INTERVALS = [0, 4, 7]\n",
    "MINOR_INTERVALS = [0, 3, 7]\n",
    "\n",
    "CHORD_TEMPLATES = {}\n",
    "for i, note in enumerate(NOTE_NAMES):\n",
    "    CHORD_TEMPLATES[note] = make_chord_template(i, MAJOR_INTERVALS)\n",
    "    CHORD_TEMPLATES[f\"{note}m\"] = make_chord_template(i, MINOR_INTERVALS)\n",
    "\n",
    "print(f\"Loaded {len(CHORD_TEMPLATES)} chord templates\")\n",
    "print(\"Chords:\", list(CHORD_TEMPLATES.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kaustav\\anaconda3\\envs\\personalDS\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio: 23.68s, 48000Hz, 1136640 samples\n"
     ]
    }
   ],
   "source": [
    "def extract_audio(audio_path: Path) -> tuple[np.ndarray, int]:\n",
    "    \"\"\"Load audio from mp3 file using librosa, return mono float32 array and sample rate.\"\"\"\n",
    "    import librosa\n",
    "    \n",
    "    # Load audio (sr=None keeps original sample rate, mono=True converts to mono)\n",
    "    audio_array, sr = librosa.load(str(audio_path), sr=None, mono=True)\n",
    "    \n",
    "    # Normalize to [-1, 1]\n",
    "    audio_array = audio_array.astype(np.float32)\n",
    "    max_val = np.max(np.abs(audio_array))\n",
    "    if max_val > 0:\n",
    "        audio_array = audio_array / max_val\n",
    "    \n",
    "    return audio_array, sr\n",
    "\n",
    "audio, sample_rate = extract_audio(AUDIO_PATH)\n",
    "duration = len(audio) / sample_rate\n",
    "print(f\"Audio: {duration:.2f}s, {sample_rate}Hz, {len(audio)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_to_chroma(freq: float) -> int:\n",
    "    \"\"\"Convert frequency (Hz) to pitch class (0-11). 0=C, 1=C#, ..., 9=A, 10=A#, 11=B.\"\"\"\n",
    "    if freq <= 0:\n",
    "        return -1\n",
    "    # MIDI note number: A4=440Hz is MIDI 69\n",
    "    midi = 12 * np.log2(freq / 440.0) + 69\n",
    "    return int(round(midi)) % 12\n",
    "\n",
    "def compute_chroma(spectrum: np.ndarray, freqs: np.ndarray, freq_min: float, freq_max: float) -> np.ndarray:\n",
    "    \"\"\"Compute chromagram from FFT magnitude spectrum.\"\"\"\n",
    "    chroma = np.zeros(12)\n",
    "    \n",
    "    for k, (mag, freq) in enumerate(zip(spectrum, freqs)):\n",
    "        if freq < freq_min or freq > freq_max:\n",
    "            continue\n",
    "        pitch_class = freq_to_chroma(freq)\n",
    "        if pitch_class >= 0:\n",
    "            chroma[pitch_class] += mag ** 2  # energy\n",
    "    \n",
    "    # Normalize\n",
    "    max_val = np.max(chroma)\n",
    "    if max_val > 0:\n",
    "        chroma = chroma / max_val\n",
    "    \n",
    "    return chroma\n",
    "\n",
    "def match_chord(chroma: np.ndarray) -> tuple[str, float]:\n",
    "    \"\"\"Match chroma vector to best chord template. Returns (chord_name, confidence).\"\"\"\n",
    "    best_score = -1\n",
    "    best_chord = \"?\"\n",
    "    \n",
    "    for name, template in CHORD_TEMPLATES.items():\n",
    "        # Cosine similarity\n",
    "        score = np.dot(chroma, template) / (np.linalg.norm(chroma) * np.linalg.norm(template) + 1e-8)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_chord = name\n",
    "    \n",
    "    return best_chord, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fft_frame(audio: np.ndarray, center: int, window_size: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Compute FFT magnitude for a single frame centered at sample 'center'.\"\"\"\n",
    "    half = window_size // 2\n",
    "    start = max(0, center - half)\n",
    "    end = min(len(audio), center + half)\n",
    "    \n",
    "    # Extract and zero-pad if needed\n",
    "    segment = np.zeros(window_size)\n",
    "    actual_start = half - (center - start)\n",
    "    segment[actual_start:actual_start + (end - start)] = audio[start:end]\n",
    "    \n",
    "    # Apply Hanning window\n",
    "    segment = segment * np.hanning(window_size)\n",
    "    \n",
    "    # FFT (positive frequencies only)\n",
    "    spectrum = np.abs(np.fft.rfft(segment))\n",
    "    freqs = np.fft.rfftfreq(window_size, 1.0 / sample_rate)\n",
    "    \n",
    "    return spectrum, freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note ranges computed for 12 pitch classes\n"
     ]
    }
   ],
   "source": [
    "def get_note_freq_ranges() -> list[tuple[str, float, float]]:\n",
    "    \"\"\"Get frequency ranges for each pitch class (covering guitar range ~80-1200 Hz).\"\"\"\n",
    "    ranges = []\n",
    "    \n",
    "    for pitch_class in range(12):\n",
    "        note_name = NOTE_NAMES[pitch_class]\n",
    "        # Collect all frequencies for this pitch class in our range\n",
    "        note_freqs = []\n",
    "        \n",
    "        for octave in range(2, 6):  # C2 to B5\n",
    "            midi = pitch_class + (octave + 1) * 12  # C4 = midi 60\n",
    "            freq = 440.0 * (2 ** ((midi - 69) / 12))\n",
    "            if FREQ_MIN <= freq <= FREQ_MAX:\n",
    "                # Frequency bin width: half semitone below to half semitone above\n",
    "                freq_low = freq * (2 ** (-0.5/12))\n",
    "                freq_high = freq * (2 ** (0.5/12))\n",
    "                note_freqs.append((freq_low, freq_high, freq))\n",
    "        \n",
    "        if note_freqs:\n",
    "            ranges.append((note_name, pitch_class, note_freqs))\n",
    "    \n",
    "    return ranges\n",
    "\n",
    "NOTE_FREQ_RANGES = get_note_freq_ranges()\n",
    "print(f\"Note ranges computed for {len(NOTE_FREQ_RANGES)} pitch classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames: 710\n",
      "Samples per frame: 1600.9\n",
      "Precomputed frame 0/710\n",
      "Precomputed frame 30/710\n",
      "Precomputed frame 60/710\n",
      "Precomputed frame 90/710\n",
      "Precomputed frame 120/710\n",
      "Precomputed frame 150/710\n",
      "Precomputed frame 180/710\n",
      "Precomputed frame 210/710\n",
      "Precomputed frame 240/710\n",
      "Precomputed frame 270/710\n",
      "Precomputed frame 300/710\n",
      "Precomputed frame 330/710\n",
      "Precomputed frame 360/710\n",
      "Precomputed frame 390/710\n",
      "Precomputed frame 420/710\n",
      "Precomputed frame 450/710\n",
      "Precomputed frame 480/710\n",
      "Precomputed frame 510/710\n",
      "Precomputed frame 540/710\n",
      "Precomputed frame 570/710\n",
      "Precomputed frame 600/710\n",
      "Precomputed frame 630/710\n",
      "Precomputed frame 660/710\n",
      "Precomputed frame 690/710\n",
      "Precomputation done!\n"
     ]
    }
   ],
   "source": [
    "# Precompute all frames\n",
    "total_frames = int(duration * FPS)\n",
    "samples_per_frame = len(audio) / total_frames\n",
    "\n",
    "print(f\"Total frames: {total_frames}\")\n",
    "print(f\"Samples per frame: {samples_per_frame:.1f}\")\n",
    "\n",
    "# Precompute FFT and chroma for each frame\n",
    "frame_data = []\n",
    "for i in range(total_frames):\n",
    "    center_sample = int(i * samples_per_frame)\n",
    "    spectrum, freqs = compute_fft_frame(audio, center_sample, WINDOW_SIZE)\n",
    "    chroma = compute_chroma(spectrum, freqs, FREQ_MIN, FREQ_MAX)\n",
    "    \n",
    "    # Calculate volume (RMS) for this window\n",
    "    half = WINDOW_SIZE // 2\n",
    "    start = max(0, center_sample - half)\n",
    "    end = min(len(audio), center_sample + half)\n",
    "    window_audio = audio[start:end]\n",
    "    volume = np.sqrt(np.mean(window_audio ** 2)) if len(window_audio) > 0 else 0\n",
    "    \n",
    "    # Only detect chord if volume is above threshold\n",
    "    is_silent = volume < VOLUME_THRESHOLD\n",
    "    if is_silent:\n",
    "        chord, confidence = '-', 0.0\n",
    "    else:\n",
    "        chord, confidence = match_chord(chroma)\n",
    "    \n",
    "    frame_data.append({\n",
    "        'spectrum': spectrum,\n",
    "        'freqs': freqs,\n",
    "        'chroma': chroma,\n",
    "        'chord': chord,\n",
    "        'confidence': confidence,\n",
    "        'center_sample': center_sample,\n",
    "        'volume': volume,\n",
    "        'is_silent': is_silent\n",
    "    })\n",
    "    \n",
    "    if i % 30 == 0:\n",
    "        print(f\"Precomputed frame {i}/{total_frames}\")\n",
    "\n",
    "print(\"Precomputation done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color scheme - LIGHT MODE\n",
    "BG_COLOR = \"#ffffff\"\n",
    "TEXT_COLOR = \"#1a1a1a\"\n",
    "WAVEFORM_COLOR = \"#2563eb\"  # blue\n",
    "FFT_COLOR = \"#2563eb\"  # blue\n",
    "DETECTED_COLOR = np.array([22, 163, 74]) / 255  # green for detected notes\n",
    "UNDETECTED_COLOR = np.array([220, 38, 38]) / 255  # red for undetected\n",
    "SILENT_COLOR = (0.6, 0.6, 0.6)  # gray for silent\n",
    "\n",
    "# Waveform window in samples\n",
    "WAVEFORM_SAMPLES = int(WAVEFORM_WINDOW * sample_rate)\n",
    "\n",
    "def lerp_color(val, color_low, color_high):\n",
    "    \"\"\"Linearly interpolate between two colors based on val (0-1).\"\"\"\n",
    "    val = np.clip(val, 0, 1)\n",
    "    return tuple(color_low + val * (color_high - color_low))\n",
    "\n",
    "def get_band_energy(spectrum, freqs, freq_low, freq_high):\n",
    "    \"\"\"Get normalized energy in a specific frequency band.\"\"\"\n",
    "    mask = (freqs >= freq_low) & (freqs <= freq_high)\n",
    "    if not np.any(mask):\n",
    "        return 0.0\n",
    "    return np.sum(spectrum[mask] ** 2)\n",
    "\n",
    "def render_frame(fig, axes, frame_idx: int, data: dict, audio: np.ndarray, sample_rate: int) -> np.ndarray:\n",
    "    \"\"\"Render a single frame.\"\"\"\n",
    "    ax_title, ax_wave, ax_fft, ax_chord, ax_blank = axes\n",
    "    \n",
    "    # Clear all axes\n",
    "    for ax in axes:\n",
    "        ax.clear()\n",
    "        ax.set_facecolor(BG_COLOR)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "    \n",
    "    # 1. Title\n",
    "    ax_title.text(0.5, 0.5, \"Chord Detection via STFT\\n(Short-Time Fourier Transform)\", ha=\"center\", va=\"center\",\n",
    "                  color=TEXT_COLOR, fontsize=36, fontweight=\"bold\", transform=ax_title.transAxes)\n",
    "    \n",
    "    # 2. Dynamic waveform (current window only, like an oscilloscope)\n",
    "    center_sample = data['center_sample']\n",
    "    half_window = WAVEFORM_SAMPLES // 2\n",
    "    wave_start = max(0, center_sample - half_window)\n",
    "    wave_end = min(len(audio), center_sample + half_window)\n",
    "    \n",
    "    # Extract current waveform segment\n",
    "    wave_segment = audio[wave_start:wave_end]\n",
    "    \n",
    "    # Pad if necessary (at start/end of audio)\n",
    "    if len(wave_segment) < WAVEFORM_SAMPLES:\n",
    "        if wave_start == 0:\n",
    "            wave_segment = np.pad(wave_segment, (WAVEFORM_SAMPLES - len(wave_segment), 0))\n",
    "        else:\n",
    "            wave_segment = np.pad(wave_segment, (0, WAVEFORM_SAMPLES - len(wave_segment)))\n",
    "    \n",
    "    # Downsample for display\n",
    "    display_samples = 500\n",
    "    step = max(1, len(wave_segment) // display_samples)\n",
    "    wave_display = wave_segment[::step]\n",
    "    wave_x = np.linspace(0, 1, len(wave_display))\n",
    "    \n",
    "    ax_wave.fill_between(wave_x, wave_display, -wave_display, alpha=0.4, color=WAVEFORM_COLOR)\n",
    "    ax_wave.plot(wave_x, wave_display, color=WAVEFORM_COLOR, linewidth=1.5)\n",
    "    ax_wave.set_xlim(0, 1)\n",
    "    ax_wave.set_ylim(-1, 1)\n",
    "    \n",
    "    # Center line\n",
    "    ax_wave.axhline(0, color=TEXT_COLOR, linewidth=0.5, alpha=0.3)\n",
    "    ax_wave.set_title(\"Audio Waveform (Real-Time)\", color=TEXT_COLOR, fontsize=20, pad=5)\n",
    "    \n",
    "    # 3. FFT with note overlays\n",
    "    spectrum = data['spectrum']\n",
    "    freqs = data['freqs']\n",
    "    chroma = data['chroma']\n",
    "    is_silent = data.get('is_silent', False)\n",
    "    \n",
    "    # Limit to frequency range of interest\n",
    "    freq_mask = (freqs >= FREQ_MIN) & (freqs <= FREQ_MAX)\n",
    "    plot_freqs = freqs[freq_mask]\n",
    "    plot_spectrum = spectrum[freq_mask]\n",
    "    \n",
    "    # Normalize spectrum for display\n",
    "    plot_spectrum = plot_spectrum / (np.max(plot_spectrum) + 1e-8)\n",
    "    \n",
    "    # Compute energy for each individual frequency band (for per-octave coloring)\n",
    "    band_energies = []\n",
    "    for note_name, pitch_class, freq_ranges in NOTE_FREQ_RANGES:\n",
    "        for freq_low, freq_high, center_freq in freq_ranges:\n",
    "            if freq_high <= FREQ_MAX:\n",
    "                energy = get_band_energy(spectrum, freqs, freq_low, freq_high)\n",
    "                band_energies.append(energy)\n",
    "    \n",
    "    # Normalize band energies\n",
    "    max_band_energy = max(band_energies) if band_energies else 1.0\n",
    "    if max_band_energy > 0:\n",
    "        band_energies_norm = [e / max_band_energy for e in band_energies]\n",
    "    else:\n",
    "        band_energies_norm = [0.0] * len(band_energies)\n",
    "    \n",
    "    # Draw note frequency bands with per-octave coloring\n",
    "    band_idx = 0\n",
    "    for note_name, pitch_class, freq_ranges in NOTE_FREQ_RANGES:\n",
    "        is_sharp = '#' in note_name\n",
    "        \n",
    "        for freq_low, freq_high, center_freq in freq_ranges:\n",
    "            if freq_high <= FREQ_MAX:\n",
    "                # Use per-band energy instead of chroma\n",
    "                band_val = band_energies_norm[band_idx] if not is_silent else 0\n",
    "                band_color = lerp_color(band_val, UNDETECTED_COLOR, DETECTED_COLOR)\n",
    "                alpha = 0.2 + 0.5 * band_val  # alpha ranges from 0.2 to 0.7\n",
    "                \n",
    "                ax_fft.axvspan(freq_low, freq_high, alpha=alpha, color=band_color)\n",
    "                # Label the note at the center of the band\n",
    "                ax_fft.text(center_freq, 1.02 + is_sharp*0.06, note_name, ha='center', va='bottom',\n",
    "                           fontsize=14, color=band_color, fontweight='bold')\n",
    "                band_idx += 1\n",
    "    \n",
    "    # Draw FFT magnitude\n",
    "    ax_fft.fill_between(plot_freqs, plot_spectrum, alpha=0.5, color=FFT_COLOR)\n",
    "    ax_fft.plot(plot_freqs, plot_spectrum, color=FFT_COLOR, linewidth=1.0)\n",
    "    ax_fft.set_xlim(FREQ_MIN, FREQ_MAX)\n",
    "    ax_fft.set_ylim(0, 1.15)  # Slightly more room for labels\n",
    "    ax_fft.set_title(\"Frequency Spectrum (K-Space)\", color=TEXT_COLOR, fontsize=20, pad=10)\n",
    "    \n",
    "    # Add frequency labels\n",
    "    ax_fft.set_xlabel(\"Frequency (Hz, Log Scale)\", color=TEXT_COLOR, fontsize=16)\n",
    "    ax_fft.tick_params(axis='x', colors=TEXT_COLOR, labelsize=20)\n",
    "    ax_fft.set_xscale('log')\n",
    "    ax_fft.set_xticks([100, 200, 400, 800])\n",
    "    ax_fft.set_xticklabels([100, 200, 400, 800])\n",
    "    for spine in ['bottom']:\n",
    "        ax_fft.spines[spine].set_visible(True)\n",
    "        ax_fft.spines[spine].set_color(TEXT_COLOR)\n",
    "    \n",
    "    # 4. Detected chord\n",
    "    chord = data['chord']\n",
    "    confidence = data['confidence']\n",
    "    if is_silent:\n",
    "        chord_color = SILENT_COLOR\n",
    "        conf_text = \"(silent)\"\n",
    "    else:\n",
    "        chord_color = lerp_color(confidence, UNDETECTED_COLOR, DETECTED_COLOR)\n",
    "        conf_text = f\"confidence: {confidence:.0%}\"\n",
    "    \n",
    "    ax_chord.text(0.5, 0.6, chord, ha=\"center\", va=\"center\",\n",
    "                  color=chord_color, fontsize=72, fontweight=\"bold\", transform=ax_chord.transAxes)\n",
    "    ax_chord.text(0.5, 0.15, conf_text, ha=\"center\", va=\"center\",\n",
    "                  color=TEXT_COLOR, fontsize=24, alpha=0.7, transform=ax_chord.transAxes)\n",
    "    \n",
    "    # 5. Blank area for video (just label it)\n",
    "    ax_blank.text(0.5, 0.5, \"[Video Overlay Area]\", ha=\"center\", va=\"center\",\n",
    "                  color=TEXT_COLOR, fontsize=24, alpha=0.2, transform=ax_blank.transAxes)\n",
    "    \n",
    "    # Render to array\n",
    "    fig.canvas.draw()\n",
    "    w, h = fig.canvas.get_width_height()\n",
    "    buf = fig.canvas.buffer_rgba()\n",
    "    rgba = np.frombuffer(buf, dtype=np.uint8).reshape((h, w, 4))\n",
    "    return np.ascontiguousarray(rgba[..., :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output will be saved to: C:\\Users\\kaustav\\OneDrive - Microsoft\\Documents\\Code\\Fun\\February2026\\FourierChord\\out\\chord_detection.mp4\n"
     ]
    }
   ],
   "source": [
    "# Setup figure (9:16 portrait, 1080x1920)\n",
    "dpi = 100\n",
    "fig = plt.figure(figsize=(1080/dpi, 1920/dpi), dpi=dpi, facecolor=BG_COLOR)\n",
    "\n",
    "# Layout: Title (6%), Waveform (14%), FFT (38%), Chord (12%), Blank (30%)\n",
    "gs = GridSpec(5, 1, figure=fig, height_ratios=[0.6, 1.4, 3.0, 1.2, 3.8], hspace=0.24,\n",
    "              top=0.98, bottom=0.02, left=0.05, right=0.95)\n",
    "ax_title = fig.add_subplot(gs[0])\n",
    "ax_wave = fig.add_subplot(gs[1])\n",
    "ax_fft = fig.add_subplot(gs[2])\n",
    "ax_chord = fig.add_subplot(gs[3])\n",
    "ax_blank = fig.add_subplot(gs[4])\n",
    "axes = (ax_title, ax_wave, ax_fft, ax_chord, ax_blank)\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output will be saved to: {OUTPUT_PATH.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 0/710 - Chord: - (0%)\n",
      "Frame 30/710 - Chord: Dm (85%)\n",
      "Frame 60/710 - Chord: Dm (85%)\n",
      "Frame 90/710 - Chord: G (71%)\n",
      "Frame 120/710 - Chord: G (65%)\n",
      "Frame 150/710 - Chord: G (67%)\n",
      "Frame 180/710 - Chord: Em (75%)\n",
      "Frame 210/710 - Chord: C (79%)\n",
      "Frame 240/710 - Chord: C (83%)\n",
      "Frame 270/710 - Chord: C (79%)\n",
      "Frame 300/710 - Chord: Am (85%)\n",
      "Frame 330/710 - Chord: Am (93%)\n",
      "Frame 360/710 - Chord: Am (88%)\n",
      "Frame 390/710 - Chord: Dm (95%)\n",
      "Frame 420/710 - Chord: Dm (93%)\n",
      "Frame 450/710 - Chord: D (65%)\n",
      "Frame 480/710 - Chord: G (93%)\n",
      "Frame 510/710 - Chord: G (83%)\n",
      "Frame 540/710 - Chord: Em (70%)\n",
      "Frame 570/710 - Chord: Cm (64%)\n",
      "Frame 600/710 - Chord: C (86%)\n",
      "Frame 630/710 - Chord: F#m (64%)\n",
      "Frame 660/710 - Chord: Am (76%)\n",
      "Frame 690/710 - Chord: Am (69%)\n",
      "\n",
      "Video (no audio) saved to: C:\\Users\\kaustav\\OneDrive - Microsoft\\Documents\\Code\\Fun\\February2026\\FourierChord\\out\\chord_detection_video_only.mp4\n",
      "\n",
      "Using ffmpeg from: c:\\Users\\kaustav\\anaconda3\\envs\\personalDS\\lib\\site-packages\\imageio_ffmpeg\\binaries\\ffmpeg-win-x86_64-v7.1.exe\n",
      "Adding audio to video...\n",
      "\n",
      "Final video with audio saved to: C:\\Users\\kaustav\\OneDrive - Microsoft\\Documents\\Code\\Fun\\February2026\\FourierChord\\out\\chord_detection.mp4\n"
     ]
    }
   ],
   "source": [
    "# Render video (without audio first)\n",
    "VIDEO_ONLY_PATH = Path(\"out/chord_detection_video_only.mp4\")\n",
    "\n",
    "writer = imageio.get_writer(\n",
    "    str(VIDEO_ONLY_PATH), fps=FPS, codec=\"libx264\",\n",
    "    macro_block_size=None,\n",
    "    ffmpeg_params=[\"-pix_fmt\", \"yuv420p\", \"-crf\", \"18\"]\n",
    ")\n",
    "\n",
    "try:\n",
    "    for i, data in enumerate(frame_data):\n",
    "        frame = render_frame(fig, axes, i, data, audio, sample_rate)\n",
    "        writer.append_data(frame)\n",
    "        \n",
    "        if i % 30 == 0:\n",
    "            print(f\"Frame {i}/{total_frames} - Chord: {data['chord']} ({data['confidence']:.0%})\")\n",
    "finally:\n",
    "    writer.close()\n",
    "    plt.close(fig)\n",
    "\n",
    "print(f\"\\nVideo (no audio) saved to: {VIDEO_ONLY_PATH.resolve()}\")\n",
    "\n",
    "# Combine video with audio using imageio-ffmpeg\n",
    "import imageio_ffmpeg\n",
    "import subprocess\n",
    "\n",
    "ffmpeg_exe = imageio_ffmpeg.get_ffmpeg_exe()\n",
    "print(f\"\\nUsing ffmpeg from: {ffmpeg_exe}\")\n",
    "print(\"Adding audio to video...\")\n",
    "\n",
    "subprocess.run([\n",
    "    ffmpeg_exe, \"-y\",\n",
    "    \"-i\", str(VIDEO_ONLY_PATH),\n",
    "    \"-i\", str(AUDIO_PATH),\n",
    "    \"-c:v\", \"copy\",\n",
    "    \"-c:a\", \"aac\",\n",
    "    \"-shortest\",\n",
    "    str(OUTPUT_PATH)\n",
    "], check=True)\n",
    "\n",
    "print(f\"\\nFinal video with audio saved to: {OUTPUT_PATH.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personalDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
